---
title: "R Notebook"
output: html_notebook
---


```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(line = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()

mode(original_books$text)
```

```{r}
library(tidytext)
tidy_books <- original_books %>%
  unnest_tokens(word, text)
tidy_books
```

```{r}
cleaned_books <- tidy_books %>%
  anti_join(get_stopwords())

cleaned_books %>%
  count(word, sort = TRUE) 
```

```{r}

nrcjoy <- get_sentiments("nrc") %>%
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  semi_join(nrcjoy) %>%
  count(word, sort = TRUE)
```

```{r}
library(tidyr)
bing <- get_sentiments("bing")

janeaustensentiment <- tidy_books %>%
  inner_join(bing) %>%
  count(book, index = line %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

library(ggplot2)

ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

```{r}
bing_word_counts <- tidy_books %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE)

bing_word_counts
```

```{r}
library(wordcloud)

cleaned_books %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r}
library(reshape2)

tidy_books %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
```

```{r}
PandP_sentences <- data_frame(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")
PandP_sentences$sentence[2]
```

```{r}
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```

```{r}
library(DT)
datatable(iris, options = list(pageLength = 5))
```

```{r}
if(!nzchar(system.file(package = "corpus.JSS.papers"))) {
 templib <- tempfile(); dir.create(templib)
 install.packages("corpus.JSS.papers", lib = templib,
 repos = "https://datacube.wu.ac.at/",
 type = "source")
 data("JSS_papers", package = "corpus.JSS.papers",
 lib.loc = templib)
 } else {
 data("JSS_papers", package = "corpus.JSS.papers")
 }
```

```{r}
library("OAIHarvester")
x <- oaih_list_records("http://www.jstatsoft.org/oai", se = "jss:ART")
x <- x[vapply(x[, "metadata"], length, 1L) > 0L, ]
JSS_papers <- oaih_transform(x[, "metadata"])
JSS_papers <- JSS_papers[order(as.Date(unlist(JSS_papers[, "date"]))), ]
JSS_papers <- JSS_papers[JSS_papers[,"date"] < "2010-08-05",]
JSS_papers <- JSS_papers[sapply(JSS_papers[, "description"],
 Encoding) == "unknown",]
```

```{r}
class(unlist(JSS_papers[, "description"]))
library("tm")
corpus <- Corpus(VectorSource(unlist(JSS_papers[,"description"])))
JSS_dtm <- DocumentTermMatrix(corpus,
 control = list(stemming = TRUE, stopwords = TRUE, minWordLength = 3,
 removeNumbers = TRUE, removePunctuation = TRUE))
dim(JSS_dtm)
library("slam")
summary(col_sums(JSS_dtm))
term_tfidf <-
 tapply(JSS_dtm$v/row_sums(JSS_dtm)[JSS_dtm$i], JSS_dtm$j, mean) *
 log2(nDocs(JSS_dtm)/col_sums(JSS_dtm > 0))
summary(term_tfidf)
JSS_dtm <- JSS_dtm[,term_tfidf >= 0.1]
JSS_dtm <- JSS_dtm[row_sums(JSS_dtm) > 0,]
summary(col_sums(JSS_dtm))
```

```{r}
library("topicmodels")
k <- 30
SEED <- 2010
jss_TM <-LDA(JSS_dtm, k = k, control = list(seed = SEED))
```

```{r}
topics(jss_TM,1)
```



